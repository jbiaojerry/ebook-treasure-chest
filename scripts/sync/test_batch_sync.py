#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æµ‹è¯•è„šæœ¬ï¼šæ ¹æ®ä¹¦ç±IDæ‰¹é‡å¤„ç†å¹¶ç”Ÿæˆmdæ–‡ä»¶
æµ‹è¯•èŒƒå›´ï¼šID 1-1000
"""

import asyncio
import aiohttp
import time
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set
import json
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from parse_book_detail_enhanced import parse_book_detail_enhanced

# å°è¯•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
import os
try:
    from config import BOOK_SITE_DOMAIN
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆGitHub Actionsä¼šä½¿ç”¨è¿™ç§æ–¹å¼ï¼‰
    BOOK_SITE_DOMAIN = os.getenv("BOOK_SITE_DOMAIN", "")
    if not BOOK_SITE_DOMAIN:
        print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ config.pyï¼Œä¹Ÿæœªè®¾ç½®ç¯å¢ƒå˜é‡ BOOK_SITE_DOMAIN")
        print("   æœ¬åœ°è¿è¡Œï¼šè¯·å¤åˆ¶ config.py.example ä¸º config.py å¹¶é…ç½® BOOK_SITE_DOMAIN")
        print("   GitHub Actionsï¼šè¯·åœ¨ Repository Settings > Secrets ä¸­æ·»åŠ  BOOK_SITE_DOMAIN")
        print("   æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export BOOK_SITE_DOMAIN='https://www.dushupai.com'")
        sys.exit(1)

# é…ç½®
BASE_URL = f"{BOOK_SITE_DOMAIN}/book-content-{{}}.html"
# è¾“å‡ºç›®å½•ï¼šæ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯æµ‹è¯•ç›®å½•è¿˜æ˜¯æ­£å¼ç›®å½•
OUTPUT_DIR_ENV = os.getenv("OUTPUT_DIR", "md_test")
if OUTPUT_DIR_ENV == "md":
    OUTPUT_DIR = Path(__file__).parent.parent.parent / "md"  # æ­£å¼ç›®å½•
else:
    OUTPUT_DIR = Path(__file__).parent.parent.parent / "md_test"  # æµ‹è¯•ç›®å½•
PROCESSED_IDS_FILE = OUTPUT_DIR / "processed_ids.json"
STATS_FILE = OUTPUT_DIR / "stats.json"
MAX_BOOK_ID_FILE = OUTPUT_DIR / "max_book_id.json"  # è®°å½•æœ€å¤§ä¹¦ç±ID

# å¹¶å‘é…ç½®
MAX_CONCURRENT = 20  # æœ€å¤§å¹¶å‘æ•°
REQUEST_DELAY = 0.5  # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰


def sanitize_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦
    
    Args:
        filename: åŸå§‹æ–‡ä»¶å
    
    Returns:
        æ¸…ç†åçš„æ–‡ä»¶å
    """
    # æ›¿æ¢ä¸åˆæ³•å­—ç¬¦
    invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
    for char in invalid_chars:
        filename = filename.replace(char, '_')
    
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    filename = filename.strip(' .')
    
    # é™åˆ¶é•¿åº¦
    if len(filename) > 200:
        filename = filename[:200]
    
    return filename or "æœªå‘½å"


def generate_md_file(tag_name: str, books: List[Dict], output_dir: Path) -> Path:
    """
    ç”ŸæˆMarkdownæ–‡ä»¶
    
    Args:
        tag_name: æ ‡ç­¾åç§°
        books: ä¹¦ç±åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç†æ–‡ä»¶å
    safe_filename = sanitize_filename(tag_name)
    file_path = output_dir / f"{safe_filename}.md"
    
    # è¿‡æ»¤æœ‰æ•ˆä¹¦ç±ï¼ˆå¿…é¡»æœ‰ä¹¦åå’Œä¸‹è½½é“¾æ¥ï¼‰
    valid_books = []
    for book in books:
        title = book.get('title', '').strip()
        # ä¼˜å…ˆä½¿ç”¨å®é™…ä¸‹è½½é“¾æ¥ï¼ˆè¯šé€šç½‘ç›˜é“¾æ¥ï¼‰ï¼Œé¿å…ä½¿ç”¨ä¸‹è½½é¡µé¢é“¾æ¥ï¼ˆåŒ…å«æ•æ„ŸåŸŸåï¼‰
        download_url = book.get('download_url', '').strip()
        
        # å¦‚æœæ²¡æœ‰å®é™…ä¸‹è½½é“¾æ¥ï¼Œè·³è¿‡è¯¥ä¹¦ï¼ˆä¸åŒ…å«ä¸‹è½½é¡µé¢é“¾æ¥ä»¥ä¿æŠ¤éšç§ï¼‰
        if not download_url:
            continue
        
        if title and download_url:
            valid_books.append({
                'title': title,
                'author': book.get('author', 'æœªçŸ¥').strip() or 'æœªçŸ¥',
                'download_url': download_url
            })
    
    if not valid_books:
        return None
    
    # ç”ŸæˆMarkdownå†…å®¹
    lines = []
    lines.append("| ä¹¦å | ä½œè€… | epub/mobi/azw3 |")
    lines.append("| --- | --- | --- |")
    
    for book in valid_books:
        # è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦
        title_escaped = book['title'].replace('|', '\\|')
        author_escaped = book['author'].replace('|', '\\|')
        # ä¿®æ”¹ä¸‹è½½é“¾æ¥ï¼šå°† ?pwd= æ”¹æˆ ?p=
        download_url = book['download_url'].replace('?pwd=', '?p=')
        
        # éšç§ä¿æŠ¤ï¼šå¦‚æœä¸‹è½½é“¾æ¥åŒ…å«æ•æ„ŸåŸŸåï¼Œç§»é™¤è¯¥ä¹¦ç±ï¼ˆä¸ç”Ÿæˆåˆ°mdæ–‡ä»¶ï¼‰
        # è¿™æ ·å¯ä»¥ç¡®ä¿GitHubä»“åº“ä¸­ä¸åŒ…å«æ•æ„ŸåŸŸå
        if download_url and BOOK_SITE_DOMAIN in download_url:
            # è·³è¿‡åŒ…å«æ•æ„ŸåŸŸåçš„é“¾æ¥ï¼ˆé€šå¸¸æ˜¯ä¸‹è½½é¡µé¢é“¾æ¥ï¼‰
            # åªä¿ç•™è¯šé€šç½‘ç›˜çš„å®é™…ä¸‹è½½é“¾æ¥
            continue
        
        download_link = f"[ä¸‹è½½]({download_url})"
        
        lines.append(f"| {title_escaped} | {author_escaped} | {download_link} |")
    
    # å†™å…¥æ–‡ä»¶
    file_path.write_text('\n'.join(lines), encoding='utf-8')
    return file_path


async def fetch_book_async(session: aiohttp.ClientSession, book_id: int, semaphore: asyncio.Semaphore) -> Dict:
    """
    å¼‚æ­¥è·å–ä¹¦ç±ä¿¡æ¯
    
    Args:
        session: aiohttpä¼šè¯
        book_id: ä¹¦ç±ID
        semaphore: ä¿¡å·é‡æ§åˆ¶å¹¶å‘
    
    Returns:
        ä¹¦ç±ä¿¡æ¯å­—å…¸ï¼Œå¤±è´¥è¿”å›None
    """
    async with semaphore:
        url = BASE_URL.format(book_id)
        
        try:
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(REQUEST_DELAY)
            
            # ä½¿ç”¨åŒæ­¥è¯·æ±‚ï¼ˆå› ä¸ºparse_book_detail_enhancedæ˜¯åŒæ­¥çš„ï¼‰
            # åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¯ä»¥æ”¹ä¸ºå¼‚æ­¥ç‰ˆæœ¬
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, parse_book_detail_enhanced, url)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–åˆ°ä¹¦åï¼ˆåˆ¤æ–­ä¹¦ç±æ˜¯å¦å­˜åœ¨ï¼‰
            if result.get('title'):
                result['book_id'] = str(book_id)
                return result
            else:
                return None
                
        except Exception as e:
            print(f"âŒ å¤„ç†ä¹¦ç±ID {book_id} æ—¶å‡ºé”™: {e}")
            return None


async def batch_process_books(book_ids: List[int]) -> Dict[str, List[Dict]]:
    """
    æ‰¹é‡å¤„ç†ä¹¦ç±
    
    Args:
        book_ids: ä¹¦ç±IDåˆ—è¡¨
    
    Returns:
        æŒ‰æ ‡ç­¾åˆ†ç±»çš„ä¹¦ç±å­—å…¸ {tag: [books]}
    """
    # æŒ‰æ ‡ç­¾åˆ†ç±»çš„ä¹¦ç±
    books_by_tag = defaultdict(list)
    
    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    
    # åˆ›å»ºaiohttpä¼šè¯
    async with aiohttp.ClientSession() as session:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [fetch_book_async(session, book_id, semaphore) for book_id in book_ids]
        
        # å¤„ç†ç»“æœ
        completed = 0
        total = len(tasks)
        
        for coro in asyncio.as_completed(tasks):
            book_data = await coro
            completed += 1
            
            if book_data:
                # æŒ‰æ ‡ç­¾åˆ†ç±»
                tags = book_data.get('tags', [])
                if tags:
                    for tag in tags:
                        books_by_tag[tag].append(book_data)
                else:
                    # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä½¿ç”¨åˆ†ç±»ä½œä¸ºæ ‡ç­¾
                    category = book_data.get('category', 'æœªåˆ†ç±»')
                    if category:
                        books_by_tag[category].append(book_data)
                    else:
                        books_by_tag['æœªåˆ†ç±»'].append(book_data)
                
                # æ˜¾ç¤ºè¿›åº¦
                if completed % 10 == 0 or completed == total:
                    print(f"ğŸ“Š è¿›åº¦: {completed}/{total} ({completed*100//total}%) - å·²æ‰¾åˆ° {len(books_by_tag)} ä¸ªæ ‡ç­¾")
            else:
                if completed % 50 == 0 or completed == total:
                    print(f"ğŸ“Š è¿›åº¦: {completed}/{total} ({completed*100//total}%)")
    
    return dict(books_by_tag)


def save_processed_ids(book_ids: Set[int]):
    """ä¿å­˜å·²å¤„ç†çš„IDåˆ—è¡¨"""
    PROCESSED_IDS_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(PROCESSED_IDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(sorted(book_ids), f, ensure_ascii=False, indent=2)


def load_processed_ids() -> Set[int]:
    """åŠ è½½å·²å¤„ç†çš„IDåˆ—è¡¨"""
    if PROCESSED_IDS_FILE.exists():
        with open(PROCESSED_IDS_FILE, 'r', encoding='utf-8') as f:
            return set(json.load(f))
    return set()


def save_stats(stats: Dict):
    """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
    with open(STATS_FILE, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)


def save_max_book_id(max_id: int):
    """ä¿å­˜æœ€å¤§ä¹¦ç±ID"""
    with open(MAX_BOOK_ID_FILE, 'w', encoding='utf-8') as f:
        json.dump({'max_book_id': max_id, 'last_updated': time.strftime('%Y-%m-%d %H:%M:%S')}, 
                 f, ensure_ascii=False, indent=2)


def load_max_book_id() -> int:
    """åŠ è½½æœ€å¤§ä¹¦ç±ID"""
    if MAX_BOOK_ID_FILE.exists():
        with open(MAX_BOOK_ID_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('max_book_id', 0)
    return 0


def generate_hot_categories_index(books_by_tag: Dict[str, List[Dict]], output_dir: Path) -> Path:
    """
    ç”Ÿæˆçƒ­é—¨åˆ†ç±»ç´¢å¼•æ–‡ä»¶ï¼ˆæŒ‰ç…§README.mdæ ¼å¼ï¼‰
    
    Args:
        books_by_tag: æŒ‰æ ‡ç­¾åˆ†ç±»çš„ä¹¦ç±å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    file_path = output_dir / "çƒ­é—¨åˆ†ç±».md"
    
    # æŒ‰ä¹¦ç±æ•°é‡æ’åº
    sorted_tags = sorted(books_by_tag.items(), key=lambda x: len(x[1]), reverse=True)
    
    # ç”Ÿæˆå†…å®¹
    lines = []
    lines.append("# ğŸ“š çƒ­é—¨åˆ†ç±»")
    lines.append("")
    lines.append("> **ğŸ’¡ ä½¿ç”¨ Ctrl+F å¿«é€Ÿæœç´¢å…³é”®è¯ï¼Œæˆ–ç‚¹å‡»ä¸‹æ–¹æ ‡ç­¾ç›´æ¥è¿›å…¥åˆ†ç±»é¡µé¢**")
    lines.append("")
    lines.append("## ğŸ”¥ çƒ­é—¨åˆ†ç±»")
    lines.append("")
    
    # æ¯è¡Œ8ä¸ªæ ‡ç­¾
    items_per_line = 8
    for i in range(0, len(sorted_tags), items_per_line):
        batch = sorted_tags[i:i + items_per_line]
        line_items = []
        
        for tag, books in batch:
            count = len(books)
            # æ¸…ç†æ–‡ä»¶åï¼ˆä¸generate_md_fileä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
            safe_filename = sanitize_filename(tag)
            link = f"- [{tag}({count})]({safe_filename}.md)"
            line_items.append(link)
        
        # ç”¨ | åˆ†éš”ï¼Œæ¯è¡Œ8ä¸ª
        line = "  | ".join(line_items)
        lines.append(line)
    
    # å†™å…¥æ–‡ä»¶
    file_path.write_text('\n'.join(lines), encoding='utf-8')
    return file_path


async def main(start_id: int = 1, end_id: int = 1000):
    """
    ä¸»å‡½æ•°
    
    Args:
        start_id: èµ·å§‹ä¹¦ç±IDï¼ˆé»˜è®¤ï¼š1ï¼‰
        end_id: ç»“æŸä¹¦ç±IDï¼ˆé»˜è®¤ï¼š1000ï¼‰
    """
    print("=" * 80)
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†ä¹¦ç±ï¼ˆID: {start_id}-{end_id}ï¼‰")
    print("=" * 80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # ç”Ÿæˆä¹¦ç±IDåˆ—è¡¨
    book_ids = list(range(start_id, end_id + 1))
    print(f"ğŸ“š å¾…å¤„ç†ä¹¦ç±æ•°é‡: {len(book_ids)}")
    
    # åŠ è½½å·²å¤„ç†çš„IDï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
    processed_ids = load_processed_ids()
    if processed_ids:
        print(f"ğŸ“‹ å·²å¤„ç†IDæ•°é‡: {len(processed_ids)}")
        book_ids = [bid for bid in book_ids if bid not in processed_ids]
        print(f"ğŸ“š å‰©ä½™å¾…å¤„ç†: {len(book_ids)}")
    
    if not book_ids:
        print("âœ… æ‰€æœ‰ä¹¦ç±å·²å¤„ç†å®Œæˆï¼")
        return
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    books_by_tag = await batch_process_books(book_ids)
    elapsed_time = time.time() - start_time
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_books = sum(len(books) for books in books_by_tag.values())
    total_tags = len(books_by_tag)
    
    print(f"\nğŸ“Š å¤„ç†å®Œæˆï¼")
    print(f"  - æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"  - æˆåŠŸå¤„ç†: {total_books} æœ¬ä¹¦")
    print(f"  - æ ‡ç­¾æ•°é‡: {total_tags} ä¸ª")
    
    # ç”Ÿæˆmdæ–‡ä»¶
    print(f"\nğŸ“ å¼€å§‹ç”ŸæˆMarkdownæ–‡ä»¶...")
    generated_files = []
    
    for tag, books in sorted(books_by_tag.items()):
        file_path = generate_md_file(tag, books, OUTPUT_DIR)
        if file_path:
            generated_files.append(str(file_path))
            # å¦‚æœæ–‡ä»¶æ•°é‡å¾ˆå¤šï¼Œå‡å°‘è¾“å‡ºé¢‘ç‡
            if len(generated_files) <= 50 or len(generated_files) % 50 == 0:
                print(f"  âœ… {tag}: {len(books)} æœ¬ä¹¦ -> {file_path.name}")
    
    print(f"\nâœ… å…±ç”Ÿæˆ {len(generated_files)} ä¸ªMarkdownæ–‡ä»¶")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_processed': total_books,
        'total_tags': total_tags,
        'generated_files': len(generated_files),
        'elapsed_time': elapsed_time,
        'tags': {tag: len(books) for tag, books in sorted(books_by_tag.items())}
    }
    save_stats(stats)
    
    # ä¿å­˜å·²å¤„ç†çš„ID
    all_processed = processed_ids | set(book_ids)
    save_processed_ids(all_processed)
    
    # ä¿å­˜æœ€å¤§ä¹¦ç±IDï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
    if book_ids:
        max_id = max(book_ids)
        save_max_book_id(max_id)
        print(f"\nğŸ“Š æœ€å¤§ä¹¦ç±ID: {max_id}ï¼ˆå·²ä¿å­˜ï¼Œç”¨äºå¢é‡æ›´æ–°ï¼‰")
    
    # ç”Ÿæˆçƒ­é—¨åˆ†ç±»ç´¢å¼•æ–‡ä»¶
    print(f"\nğŸ“ ç”Ÿæˆçƒ­é—¨åˆ†ç±»ç´¢å¼•æ–‡ä»¶...")
    hot_categories_file = generate_hot_categories_index(books_by_tag, OUTPUT_DIR)
    print(f"  âœ… çƒ­é—¨åˆ†ç±»ç´¢å¼•: {hot_categories_file.name}")
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {STATS_FILE}")
    print("=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¤„ç†ä¹¦ç±å¹¶ç”Ÿæˆmdæ–‡ä»¶')
    parser.add_argument('--start-id', type=int, default=1, help='èµ·å§‹ä¹¦ç±IDï¼ˆé»˜è®¤ï¼š1ï¼‰')
    parser.add_argument('--end-id', type=int, default=1000, help='ç»“æŸä¹¦ç±IDï¼ˆé»˜è®¤ï¼š1000ï¼‰')
    args = parser.parse_args()
    
    asyncio.run(main(args.start_id, args.end_id))
