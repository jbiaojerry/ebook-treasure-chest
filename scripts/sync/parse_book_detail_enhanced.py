#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆä¹¦ç±è¯¦æƒ…é¡µè§£ææ¨¡å—
åŠŸèƒ½ï¼šè§£æä¹¦ç±è¯¦æƒ…é¡µï¼Œæå–å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ ‡ç­¾ã€å›¾ç‰‡ç­‰ï¼‰
"""

import requests
import re
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from typing import Dict, List, Optional

# å°è¯•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
try:
    from config import BOOK_SITE_DOMAIN
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
    BOOK_SITE_DOMAIN = os.getenv("BOOK_SITE_DOMAIN", "")
    if not BOOK_SITE_DOMAIN:
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨å ä½ç¬¦ï¼ˆä½†ä¼šå¯¼è‡´åŠŸèƒ½ä¸å¯ç”¨ï¼‰
        BOOK_SITE_DOMAIN = ""


def parse_download_page(url: str) -> Optional[Dict[str, str]]:
    """
    è§£æä¸‹è½½é¡µé¢ï¼Œæå–è¯šé€šç½‘ç›˜çš„çœŸå®ä¸‹è½½é“¾æ¥
    
    Args:
        url: ä¸‹è½½é¡µé¢ URLï¼Œä¾‹å¦‚: "https://www.dushupai.com/download-book-64938.html"
    
    Returns:
        Optional[Dict]: å¦‚æœæ‰¾åˆ°è¯šé€šç½‘ç›˜ä¸‹è½½é“¾æ¥ï¼Œè¿”å›åŒ…å« download_url çš„å­—å…¸ï¼›
                       å¦‚æœä¸å­˜åœ¨è¯¥ä¸‹è½½æ–¹å¼ï¼Œè¿”å› None
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # è§£æ HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ­¥éª¤1: ç²¾ç¡®å®šä½"è¯šé€šç½‘ç›˜ä¸‹è½½"åŒºåŸŸ
        # æ–¹æ³•1: æŸ¥æ‰¾ class="source-title" ä¸”åŒ…å«"è¯šé€šç½‘ç›˜"çš„ div
        cheng_tong_title = soup.find('div', class_='source-title', string=lambda x: x and 'è¯šé€šç½‘ç›˜' in str(x) if x else False)
        
        if not cheng_tong_title:
            # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«"è¯šé€šç½‘ç›˜ä¸‹è½½"æ–‡æœ¬çš„å…ƒç´ 
            cheng_tong_text = soup.find(string=lambda x: x and 'è¯šé€šç½‘ç›˜ä¸‹è½½' in str(x) if x else False)
            if cheng_tong_text:
                # å‘ä¸ŠæŸ¥æ‰¾åŒ…å«è¯¥æ–‡æœ¬çš„ source-title div
                parent = cheng_tong_text.parent
                while parent:
                    if parent.name == 'div' and 'source-title' in str(parent.get('class', [])):
                        cheng_tong_title = parent
                        break
                    if parent.name in ['body', 'html']:
                        break
                    parent = parent.parent
        
        if not cheng_tong_title:
            # å¦‚æœæ‰¾ä¸åˆ°"è¯šé€šç½‘ç›˜ä¸‹è½½"åŒºåŸŸï¼Œè¿”å› None
            return None
        
        # æ­¥éª¤2: æ‰¾åˆ°åŒ…å«"è¯šé€šç½‘ç›˜ä¸‹è½½"çš„å®¹å™¨ï¼ˆé€šå¸¸æ˜¯ <div class="box">ï¼‰
        container = cheng_tong_title.parent
        while container:
            # å¦‚æœå½“å‰å®¹å™¨æ˜¯ boxï¼Œä½¿ç”¨å®ƒ
            if container.name == 'div' and 'box' in str(container.get('class', [])):
                break
            
            # æ£€æŸ¥å½“å‰å®¹å™¨æ˜¯å¦åŒ…å« button
            button_div = container.find('div', class_='button')
            if button_div:
                # æ‰¾åˆ°äº†åŒ…å«æŒ‰é’®çš„å®¹å™¨
                break
            
            # ç»§ç»­å‘ä¸ŠæŸ¥æ‰¾
            container = container.parent
            
            # å¦‚æœå·²ç»åˆ° body æˆ– htmlï¼Œåœæ­¢
            if not container or container.name in ['body', 'html']:
                container = None
                break
        
        if not container:
            return None
        
        # æ­¥éª¤3: åœ¨å®¹å™¨å†…æŸ¥æ‰¾"ç«‹å³ä¸‹è½½"æŒ‰é’®å¹¶æå–é“¾æ¥
        # æ–¹æ³•1: æŸ¥æ‰¾ class="button" çš„ div ä¸­çš„é“¾æ¥ï¼ˆæœ€ç²¾ç¡®ï¼‰
        button_div = container.find('div', class_='button')
        if button_div:
            download_link = button_div.find('a', href=True)
            if download_link:
                download_url = download_link.get('href', '').strip()
                # éªŒè¯æ˜¯å¦æ˜¯ ctfile.com é“¾æ¥
                if download_url and 'ctfile.com' in download_url.lower():
                    return {
                        "download_url": download_url
                    }
        
        # æ–¹æ³•2: åœ¨å®¹å™¨å†…æŸ¥æ‰¾åŒ…å«"ç«‹å³ä¸‹è½½"æ–‡æœ¬çš„é“¾æ¥
        download_links = container.find_all('a', string=lambda x: x and 'ç«‹å³ä¸‹è½½' in str(x) if x else False)
        for link in download_links:
            href = link.get('href', '').strip()
            if href and 'ctfile.com' in href.lower():
                return {
                    "download_url": href
                }
        
        # æ–¹æ³•3: åœ¨å®¹å™¨å†…æŸ¥æ‰¾æ‰€æœ‰åŒ…å« ctfile.com çš„é“¾æ¥ï¼ˆæœ€åå¤‡ç”¨ï¼‰
        ctfile_links = container.find_all('a', href=lambda x: x and 'ctfile.com' in str(x).lower() if x else False)
        if ctfile_links:
            download_url = ctfile_links[0].get('href', '').strip()
            if download_url:
                return {
                    "download_url": download_url
                }
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å› None
        return None
        
    except requests.exceptions.RequestException as e:
        return None
    except Exception as e:
        return None


def extract_book_id(url: str) -> Optional[str]:
    """
    ä»URLä¸­æå–ä¹¦ç±ID
    
    Args:
        url: ä¹¦ç±è¯¦æƒ…é¡µURLï¼Œä¾‹å¦‚: "https://www.dushupai.com/book-content-63067.html"
    
    Returns:
        ä¹¦ç±IDå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚: "63067"
    """
    match = re.search(r'book-content-(\d+)\.html', url)
    if match:
        return match.group(1)
    return None


def parse_book_detail_enhanced(url: str) -> Dict:
    """
    è§£æä¹¦ç±è¯¦æƒ…é¡µï¼Œæå–å®Œæ•´ä¿¡æ¯
    
    Args:
        url: ä¹¦ç±è¯¦æƒ…é¡µ URLï¼Œä¾‹å¦‚: "https://www.dushupai.com/book-content-63067.html"
    
    Returns:
        Dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
            - book_id: ä¹¦ç±ID
            - title: ä¹¦å
            - author: ä½œè€…ä¿¡æ¯
            - cover_image: å°é¢å›¾ç‰‡URL
            - download_page: ä¸‹è½½é¡µé¢URL
            - download_url: å®é™…ä¸‹è½½é“¾æ¥ï¼ˆéœ€è¦è¿›ä¸€æ­¥è§£æä¸‹è½½é¡µï¼‰
            - tags: æ ‡ç­¾åˆ—è¡¨
            - category: åˆ†ç±»
            - isbn: ISBNå·
            - rating: è¯„åˆ†
            - publish_date: å‘å¸ƒæ—¥æœŸ
            - description: å†…å®¹ç®€ä»‹
            - author_bio: ä½œè€…ç®€ä»‹
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    result = {
        "book_id": extract_book_id(url) or "",
        "title": "",
        "author": "æœªçŸ¥",
        "cover_image": "",
        "download_page": "",
        "download_url": "",
        "tags": [],
        "category": "",
        "isbn": "",
        "rating": "",
        "publish_date": "",
        "description": "",
        "author_bio": "",
        "formats": []
    }
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # è§£æ HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. æå–ä¹¦å
        title_elem = soup.select_one('h4.post-title')
        if title_elem:
            result["title"] = title_elem.get_text(strip=True)
        
        if not result["title"]:
            post_info = soup.select_one('.post-info')
            if post_info:
                for li in post_info.find_all('li'):
                    strong = li.find('strong')
                    if strong and 'ä¹¦å' in strong.get_text():
                        title_text = li.get_text().replace('ä¹¦åï¼š', '').strip()
                        if title_text:
                            result["title"] = title_text
                            break
        
        # 2. æå–ä½œè€…ä¿¡æ¯
        post_info = soup.select_one('.post-info')
        if post_info:
            for li in post_info.find_all('li'):
                strong = li.find('strong')
                if strong and 'ä½œè€…' in strong.get_text():
                    author_links = li.find_all('a', href=lambda x: x and 'book-author' in str(x) if x else False)
                    if author_links:
                        authors = [link.get_text(strip=True) for link in author_links if link.get_text(strip=True)]
                        if authors:
                            result["author"] = " / ".join(authors)
                    else:
                        author_text = li.get_text().replace('ä½œè€…ï¼š', '').strip()
                        if author_text:
                            result["author"] = author_text
                    break
        
        # 3. æå–å°é¢å›¾ç‰‡
        img_elem = soup.select_one('.post-content img')
        if img_elem:
            img_src = img_elem.get('src', '')
            if img_src:
                result["cover_image"] = urljoin(url, img_src)
        
        # 4. æå–ä¸‹è½½é¡µé¢URLå¹¶è§£æå®é™…ä¸‹è½½é“¾æ¥
        download_elem = soup.select_one('.post-download a')
        if download_elem:
            download_href = download_elem.get('href', '')
            if download_href:
                result["download_page"] = urljoin(url, download_href)
                
                # è§£æä¸‹è½½é¡µé¢ï¼Œè·å–è¯šé€šç½‘ç›˜çš„å®é™…ä¸‹è½½é“¾æ¥
                try:
                    download_info = parse_download_page(result["download_page"])
                    if download_info and download_info.get("download_url"):
                        result["download_url"] = download_info["download_url"]
                except Exception as e:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä¸å½±å“å…¶ä»–ä¿¡æ¯çš„æå–
                    pass
        
        # 5. æå–æ ‡ç­¾åˆ—è¡¨ï¼ˆå…³é”®åŠŸèƒ½ï¼‰
        # æ–¹æ³•1ï¼šç›´æ¥æŸ¥æ‰¾åŒ…å« book-tag çš„é“¾æ¥ï¼ˆæœ€å¯é ï¼‰
        tag_links = soup.find_all('a', href=lambda x: x and 'book-tag' in str(x) if x else False)
        if tag_links:
            tags = []
            for link in tag_links:
                tag_text = link.get_text(strip=True)
                # æ ¼å¼å¯èƒ½æ˜¯ "ä¸­å›½(5333)" æˆ– "ä¸­å›½"
                if '(' in tag_text:
                    tag_name = tag_text.split('(')[0].strip()
                else:
                    tag_name = tag_text
                
                # è¿‡æ»¤æ‰ç©ºæ ‡ç­¾å’Œæ•°å­—æ ‡ç­¾ï¼ˆå¦‚å¹´ä»½ï¼‰
                if tag_name and tag_name not in tags:
                    # è·³è¿‡çº¯æ•°å­—æ ‡ç­¾ï¼ˆå¦‚å¹´ä»½ï¼‰
                    if not tag_name.isdigit():
                        tags.append(tag_name)
            
            result["tags"] = tags
        
        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾"æ ‡ç­¾ï¼š"åé¢çš„å†…å®¹
        if not result["tags"]:
            for elem in soup.find_all(['div', 'p', 'span', 'strong']):
                text = elem.get_text()
                if 'æ ‡ç­¾' in text and ('ï¼š' in text or ':' in text):
                    # æŸ¥æ‰¾çˆ¶å…ƒç´ æˆ–å…„å¼Ÿå…ƒç´ ä¸­çš„æ ‡ç­¾é“¾æ¥
                    parent = elem.find_parent()
                    if parent:
                        tag_links = parent.find_all('a', href=lambda x: x and 'book-tag' in str(x) if x else False)
                        if tag_links:
                            tags = []
                            for link in tag_links:
                                tag_text = link.get_text(strip=True)
                                if '(' in tag_text:
                                    tag_name = tag_text.split('(')[0].strip()
                                else:
                                    tag_name = tag_text
                                if tag_name and tag_name not in tags and not tag_name.isdigit():
                                    tags.append(tag_name)
                            if tags:
                                result["tags"] = tags
                                break
        
        # 6. æå–åˆ†ç±»
        category_link = soup.find('a', href=lambda x: x and 'book-category' in str(x) if x else False)
        if category_link:
            result["category"] = category_link.get_text(strip=True)
        
        # 7. æå–å…¶ä»–ä¿¡æ¯ï¼ˆISBNã€è¯„åˆ†ã€å‘å¸ƒæ—¥æœŸç­‰ï¼‰
        if post_info:
            for li in post_info.find_all('li'):
                strong = li.find('strong')
                if not strong:
                    continue
                
                strong_text = strong.get_text()
                li_text = li.get_text()
                
                if 'ISBN' in strong_text:
                    result["isbn"] = li_text.replace('ISBNï¼š', '').replace('ISBN:', '').strip()
                elif 'è¯„åˆ†' in strong_text:
                    rating_text = li_text.replace('è¯„åˆ†ï¼š', '').replace('è¯„åˆ†:', '').strip()
                    result["rating"] = rating_text
                elif 'æ—¶é—´' in strong_text or 'æ—¥æœŸ' in strong_text:
                    date_text = li_text.replace('æ—¶é—´ï¼š', '').replace('æ—¥æœŸï¼š', '').strip()
                    result["publish_date"] = date_text
                elif 'æ ¼å¼' in strong_text:
                    formats_text = li_text.replace('æ ¼å¼ï¼š', '').replace('æ ¼å¼:', '').strip()
                    result["formats"] = [f.strip() for f in formats_text.split(',') if f.strip()]
        
        # 8. æå–å†…å®¹ç®€ä»‹
        desc_elem = soup.find(string=re.compile(r'å†…å®¹ç®€ä»‹'))
        if desc_elem:
            # æŸ¥æ‰¾ç®€ä»‹å†…å®¹ï¼ˆé€šå¸¸åœ¨"å†…å®¹ç®€ä»‹"åé¢çš„å…ƒç´ ä¸­ï¼‰
            parent = desc_elem.find_parent()
            if parent:
                # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ æˆ–çˆ¶å…ƒç´ ä¸­çš„æ–‡æœ¬
                desc_text = ""
                for sibling in parent.next_siblings:
                    if hasattr(sibling, 'get_text'):
                        desc_text = sibling.get_text(strip=True)
                        if desc_text:
                            break
                
                if not desc_text:
                    # å°è¯•ä»çˆ¶å…ƒç´ ä¸­æå–
                    desc_text = parent.get_text(strip=True)
                    # å»æ‰"å†…å®¹ç®€ä»‹ï¼š"å‰ç¼€
                    desc_text = re.sub(r'å†…å®¹ç®€ä»‹[ï¼š:]?\s*', '', desc_text)
                
                result["description"] = desc_text[:500]  # é™åˆ¶é•¿åº¦
        
        # 9. æå–ä½œè€…ç®€ä»‹
        author_bio_elem = soup.find(string=re.compile(r'ä½œè€…ç®€ä»‹'))
        if author_bio_elem:
            parent = author_bio_elem.find_parent()
            if parent:
                bio_text = ""
                for sibling in parent.next_siblings:
                    if hasattr(sibling, 'get_text'):
                        bio_text = sibling.get_text(strip=True)
                        if bio_text:
                            break
                
                if not bio_text:
                    bio_text = parent.get_text(strip=True)
                    bio_text = re.sub(r'ä½œè€…ç®€ä»‹[ï¼š:]?\s*', '', bio_text)
                
                result["author_bio"] = bio_text[:300]  # é™åˆ¶é•¿åº¦
        
        return result
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return result
    except Exception as e:
        print(f"âŒ è§£æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return result


def main():
    """ä¸»å‡½æ•°ï¼šæµ‹è¯•å¢å¼ºç‰ˆè§£æåŠŸèƒ½"""
    print("=" * 80)
    print("ğŸ“š æµ‹è¯•å¢å¼ºç‰ˆ parse_book_detail_enhanced åŠŸèƒ½")
    print("=" * 80)
    
    # æµ‹è¯• URL
    test_url = "https://www.dushupai.com/book-content-63067.html"
    
    print(f"\nğŸ“„ æµ‹è¯• URL: {test_url}\n")
    
    # è§£æä¹¦ç±è¯¦æƒ…
    result = parse_book_detail_enhanced(test_url)
    
    # æ˜¾ç¤ºç»“æœ
    print("ğŸ“‹ è§£æç»“æœï¼š")
    print("-" * 80)
    print(f"ä¹¦ç±ID: {result['book_id']}")
    print(f"ä¹¦å: {result['title']}")
    print(f"ä½œè€…: {result['author']}")
    print(f"å°é¢å›¾ç‰‡: {result['cover_image']}")
    print(f"ä¸‹è½½é¡µé¢: {result['download_page']}")
    print(f"æ ‡ç­¾: {', '.join(result['tags']) if result['tags'] else 'æ— '}")
    print(f"åˆ†ç±»: {result['category']}")
    print(f"ISBN: {result['isbn']}")
    print(f"è¯„åˆ†: {result['rating']}")
    print(f"å‘å¸ƒæ—¥æœŸ: {result['publish_date']}")
    print(f"æ ¼å¼: {', '.join(result['formats']) if result['formats'] else 'æ— '}")
    print(f"ç®€ä»‹: {result['description'][:100]}..." if result['description'] else "æ— ç®€ä»‹")
    
    # æ˜¾ç¤º JSON æ ¼å¼
    print("\nğŸ“‹ JSON æ ¼å¼ï¼š")
    import json
    print(json.dumps(result, ensure_ascii=False, indent=2))
    
    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    
    return result


if __name__ == "__main__":
    result = main()
